# Data transformation

The data extraction and transformation process took substantial efforts throughout the course of this project. In order to identify all of the games where a ‘choke’ occurred over the last 4 NBA seasons, we needed to query over the data frames of 10,000 total games played, and the event-level data of each of these games. In order to identify a ‘choke’, we checked to see if at any point during the if a team had a win probability > 90% and then ended up losing the game. Once we identified such an occurrence, we appended a row to a new dataset, which will hold all of the instances of chokes. Each row of this newly created dataset holds a game_id, the two teams who played, which of the teams was a home team vs. away team and finally which team choked. With this newly created dataset, we are able to identify the number of times each NBA team choked as well as whether they were playing at home or as a visitor. This newly created dataset is now ready to be exported from Python to R and used for analysis. 

Once we were able to identify the actual games that ended in a choke, we were interested in exploring the game-level data for these chokes. Therefore, for each game that was classified as a choke, we used its game_id (from our first dataset) to make an additional query to the nba_api to get the entire game’s play-by-play data. This data extraction process now gives us the entire game description of each game that ended in a choke over the last 4 NBA seasons. However, we needed to conduct additional transformations on this dataframe to make it more useful for our project. The main task was to convert the event_description strings to a categorical variable. This transformation then allows for us to assign a potential explanatory reason for each choke that occurred. In its original form, the API gives us the description of the play-by-play data in the form of a sentence and we wanted to convert these sentences to a single respective variable, whenever applicable. To provide an example of this transformation, a play-by-play description from the source dataset looked like: “MISS Finney-Smith 26' 3PT Pullup Jump Shot” and we wanted to classify this row as simply a “miss”. Therefore, we parsed through the dataset’s event description data and classified each event into 1 of 3 categories: miss, foul, turnover. Given that these were the most frequent mistakes that a team will typically make and result in them losing, we looked to identify all of the moments in a game where one of these 3 occurrences took place.

Further, in our analysis, we thought it would be of interest to examine player-level statistics in games that ended as a choke. Therefore, we leveraged our access to the play-by-play data and parsed over all the event descriptions to split each player's points into 2 categories: their points scored before the max win threshold and then the points scored after. This max win threshold is a crucial point in the game, since it is after this point where a team with a sizable lead begins to lose steam. This transformation would allow for us to model the difference in distributions of players points during a choke, but split by the turning point in the game. We used the same nba_api play-by-play data’s event descriptions to determine which player is scoring at each moment of the game, whether or not their team ended up as the choke and if this point was scored before or after their max win threshold. Once this dataset was split into players scores before and after the threshold point, it was relatively straightforward to overlay their density curves on a graph.  

In order to convert the player-specific data so that it can be used by R's base mosaic plot, we required some reformatting of the data that we obtained via the nbastatR package. This package returns a data frame of data tables, all of which hold a season of statistics for a given player. As a result of this format, it was necessary to make use of pivot_longer in order to expand the player data for each input. Additionally, since this package returns data in a season-by-season basis, it was necessary to perform some data aggregation, to get the mean performance of each player, at each statistic, in long format. 

While we did require additional data transformations later on in the project, overall, these initial processes took up the majority of our time in getting our data reformatted in such a way to then be exported to R for further analysis. 
