# Data transformation

The data extraction and transformation process took substantial efforts throughout the course of this project. In order to identify all of the games where a ‘choke’ occurred over the last 4 NBA seasons, we needed to query over the data frames of all 30 teams, all of which play 82 games a season, surmounting to a total of 10,000 total games played. Further, we needed to then look at the event-level data of each of these games. In order to identify a ‘choke’, we checked to see if at any point during the if a team had a win probability > 90% and then ended up losing the game. We used the nba_api's play-by-play method to pull the event-level description of each game, including both team's win probabilities at each moment. Once we identified an occurrence of a choke, we appended a row to a new data set, which will hold all of the instances of chokes. Each row of this newly created data set holds a unique game_id, the two teams who played, which of the teams was a home team vs. away team and finally which team choked. With this newly created data set, we are able to identify the number of times each NBA team choked as well as whether they were playing at home or as a visitor. This newly created data set is now ready to be exported from Python to R and used for analysis. The Python script that was used for this process can be found on our GitHub page, under PythonScripts/pull_chokes.py.

Once we were able to identify the actual games that ended in a choke, we were interested in exploring the game-level data for these chokes. Therefore, for each game that was classified as a choke, we used its game_id (from our first dataset) to make an additional query to the nba_api to get the entire game’s play-by-play data. This data extraction process now gives us the entire game description of each game that ended in a choke over the last 4 NBA seasons. Given that we only are concerned with examining the nuances of choke-ending games, we did not want to request data we did not need, and as a result only pulled the play-by-play data of the games we have already identified as chokes. However, we needed to conduct additional transformations on this data frame to make it more useful for our project. The main task was to convert the event_description strings to a categorical variable. This transformation then allows for us to assign a potential explanatory reason for each choke that occurred. In its original form, the API gives us the description of the play-by-play data in the form of a sentence and we wanted to convert these sentences to a single respective variable, whenever applicable. To provide an example of this transformation, a play-by-play description from the source dataset looked like: “MISS Finney-Smith 26' 3PT Pullup Jump Shot” and we wanted to classify this row as simply a “miss”. Therefore, we parsed through the dataset’s event description data and classified each event into 1 of 3 categories: miss, foul, turnover. Given that these were the most frequent mistakes that a team will typically make and result in them losing, we looked to identify all of the moments in a game where any of these 3 occurrences took place. The Python script that was used for this process can be found on our GitHub page, under PythonScripts/form_pbp_data.py.

Further, in our analysis, we thought it would be of interest to examine player-level statistics in games that ended as a choke. Therefore, we leveraged our access to the play-by-play data and parsed over all the event descriptions to split each player's points into 2 categories: their points scored before the max win threshold and then the points scored after. This max win threshold is a crucial point in the game, since it is after this point where a team with a sizable lead begins to lose their momentum. This transformation would allow for us to model the difference in distributions of players points during a choke, but split by the turning point in the game. We used the same nba_api play-by-play data’s event descriptions to determine which player is scoring at each moment of the game, whether or not their team ended up as the choke and if this point was scored before or after their max win threshold. Once this data set was split into players scores before and after the threshold point, it was relatively straightforward to overlay their density curves on a graph in R. The Python script that was used for this process can be found on our GitHub page, under PythonScripts/player_data.py  

In order to convert the player-specific data so that it can be used by R's base mosaic plot, we required some reformatting of the data that we obtained via the nbastatR package. This package returns a data frame of data tables, all of which hold a season of statistics for a given player. As a result of this format, it was necessary to make use of pivot_longer in order to expand the player data for each input. Additionally, since this package returns data in a season-by-season basis, it was necessary to perform some data aggregation, to get the mean performance of each player, at each statistic, in long format. For our analysis, we only wanted to look at the players who were on a high-choke and high-comeback team and as a result, we simply hard coded their names into the request for their data, however, this package has much more dynamic use cases if we were to do further data exploration. Also, since this player-specific data pulls from an extremely large data set, and this project makes a request, it is possible that on your machine you may receive the following error: "The size of the connection buffer (131072) was not large enough". If this is the case, simply increase the size of your buffer by running the following command: "Sys.setenv("VROOM_CONNECTION_SIZE"=131072*2)".

While we did require additional data transformations later on in the project, overall, these initial processes took up the majority of our time in getting our data reformatted in such a way to then be exported to R for further analysis.
